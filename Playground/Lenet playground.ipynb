{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import gzip\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open('MNIST_data/mnist.pkl.gz','rb') as ff :\n",
    "    u = pickle._Unpickler( ff )\n",
    "    u.encoding = 'latin1'\n",
    "    train, val, test = u.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "train_data = train[0]\n",
    "train_data_labels = train[1]\n",
    "test_data = test[0]\n",
    "test_data_labels = test[1]\n",
    "\n",
    "print(np.shape(train_data))\n",
    "print(np.shape(train_data_labels))\n",
    "print(np.shape(test_data))\n",
    "print(np.shape(test_data_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_one_hot = tf.one_hot(train_data_labels, depth=10)\n",
    "cls_t_one_hot = tf.one_hot(test_data_labels, depth=10)\n",
    "sess = tf.Session()\n",
    "with sess.as_default():\n",
    "    train_target = cls_one_hot.eval()\n",
    "    test_target = cls_t_one_hot.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "shfl = np.arange(train_data.shape[0])\n",
    "np.random.shuffle(shfl)\n",
    "train_data_shfl = train_data[shfl,:]\n",
    "train_target_shfl = train_target[shfl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_mean = np.mean(train_data_shfl)\n",
    "train_data_shfl2 = (train_data_shfl - train_data_mean)\n",
    "\n",
    "test_data_mean = np.mean(test_data)\n",
    "test_data2 = (test_data - test_data_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_var = np.sqrt(np.var(train_data_shfl2))\n",
    "train_data_shfl3 = (train_data_shfl2 / train_data_var)\n",
    "\n",
    "test_data_var = np.sqrt(np.var(test_data2))\n",
    "test_data3 = (test_data2 / test_data_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(np.var(train_data_shfl3))\n",
    "print(np.var(test_data3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000, 10)\n",
      "(10000, 784)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(train_data))\n",
    "print(np.shape(train_target))\n",
    "print(np.shape(test_data))\n",
    "print(np.shape(test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_y(rgb):\n",
    "    rgb_to_y_mat = np.array([[0.299], [0.587], [0.114]])\n",
    "    y = np.dot(rgb, rgb_to_y_mat)\n",
    "    y = np.reshape(y, [y.shape[0], y.shape[1], y.shape[2]])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def initialize_parameters_bnz(layers):\n",
    "    parameters = {}\n",
    "    bn_parameters = {}\n",
    "    \n",
    "    np.random.seed(10)\n",
    "    \n",
    "    num_layers = np.size(layers) - 1\n",
    "    for i in range(num_layers):\n",
    "        parameters[\"W\" + str(i+1)] = initialize_w([layers[i+1], layers[i]])\n",
    "        parameters[\"b\" + str(i+1)] = initialize_b([layers[i+1], 1])\n",
    "        bn_parameters[\"mean\" + str(i+1)] = 0.0\n",
    "        bn_parameters[\"var\" + str(i+1)] = 1.0\n",
    "    \n",
    "    return parameters, bn_parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batch_norm_forward(X):\n",
    "    mu = np.mean(X)\n",
    "    var = np.var(X)\n",
    "    X_norm = (X-mu)/np.sqrt(var+1e-10)\n",
    "    return X_norm, mu, var"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def calc_z_bnz(X, W, b, bnz=0, mean=0, variance=1):\n",
    "    Z = np.dot(W, X.T) + b\n",
    "    \n",
    "    if(bnz): # training\n",
    "        Z_norm, mu, var = batch_norm_forward(Z)\n",
    "        mean = 0.9*mean + 0.1*mu\n",
    "        variance = 0.9*variance + 0.1*var\n",
    "    else: # batch norm disabled OR inference pass\n",
    "        Z_norm = (Z-mean)/np.sqrt(variance+1e-10)\n",
    "        \n",
    "    return Z, Z_norm, mean, variance"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def forward_pass_bnz(X, parameters, num_layers, bn_parameters, bnz=0):\n",
    "    cache = {}\n",
    "    cache[\"A0\"] = X\n",
    "    A_prev = X\n",
    "    \n",
    "    for i in range(num_layers-1):\n",
    "        mean = bn_parameters[\"mean\" + str(i+1)]\n",
    "        var = bn_parameters[\"var\" + str(i+1)]\n",
    "        \n",
    "        cache[\"Z\" + str(i+1)], \\\n",
    "        cache[\"Znorm\" + str(i+1)], \\\n",
    "        bn_parameters[\"mean\" + str(i+1)], \\\n",
    "        bn_parameters[\"var\" + str(i+1)] = \\\n",
    "            calc_z_bnz(A_prev, parameters[\"W\" + str(i+1)], parameters[\"b\" + str(i+1)], bnz, mean, var)\n",
    "        \n",
    "        cache[\"A\" + str(i+1)] = calc_activation_fn(cache[\"Z\" + str(i+1)], \"relu\")\n",
    "        A_prev = cache[\"A\" + str(i+1)]\n",
    "\n",
    "    mean = bn_parameters[\"mean\" + str(num_layers)]\n",
    "    var = bn_parameters[\"var\" + str(num_layers)]\n",
    "    \n",
    "    cache[\"Z\" + str(num_layers)], \\\n",
    "    cache[\"Znorm\" + str(num_layers)], \\\n",
    "    bn_parameters[\"mean\" + str(num_layers)], \\\n",
    "    bn_parameters[\"var\" + str(num_layers)] = \\\n",
    "        calc_z_bnz(A_prev, parameters[\"W\" + str(num_layers)], parameters[\"b\" + str(num_layers)], bnz, mean, var)\n",
    "    \n",
    "    cache[\"A\" + str(num_layers)] = calc_activation_fn(cache[\"Z\" + str(num_layers)], \"softmax\")\n",
    "    \n",
    "    return cache, bn_parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def batchnorm_backward(dout, X, X_norm, mu, var):\n",
    "    N,D = X.shape\n",
    "\n",
    "    X_mu = X - mu\n",
    "    std_inv = 1. / np.sqrt(var + 1e-10)\n",
    "\n",
    "    dX_norm = dout * 1.0\n",
    "    dvar = np.sum(dX_norm * X_mu, axis=0) * -.5 * std_inv**3\n",
    "    dmu = np.sum(dX_norm * -std_inv, axis=0) + dvar * np.mean(-2. * X_mu, axis=0)\n",
    "\n",
    "    dX = (dX_norm * std_inv) + (dvar * 2 * X_mu / N) + (dmu / N)\n",
    "\n",
    "    return dX"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def backward_pass_bnz(Y, parameters, cache, num_layers, bn_parameters):\n",
    "    \n",
    "    m = Y.shape[0]\n",
    "    grads = {}\n",
    "    \n",
    "    dZ_prev = np.transpose(cache[\"A\" + str(num_layers)] - Y)\n",
    "    dZ_prev = batchnorm_backward(dZ_prev, cache[\"Z\" + str(num_layers)], cache[\"Znorm\" + str(num_layers)], \\\n",
    "                                bn_parameters[\"mean\" + str(num_layers)], bn_parameters[\"var\" + str(num_layers)])\n",
    "    grads[\"dW\" + str(num_layers)] = (1/m) * np.dot(dZ_prev, cache[\"A\" + str(num_layers-1)])\n",
    "    grads[\"db\" + str(num_layers)] = (1/m) * np.sum(dZ_prev, axis=1, keepdims=True)\n",
    "    \n",
    "    for i in range(num_layers-1, 0, -1):\n",
    "        dZ = np.transpose(np.dot(dZ_prev.T, parameters[\"W\" + str(i+1)])) * compute_relu_grad(cache[\"Z\" + str(i)])\n",
    "        dZ = batchnorm_backward(dZ, cache[\"Z\" + str(i)], cache[\"Znorm\" + str(i)], \\\n",
    "                                bn_parameters[\"mean\" + str(i)], bn_parameters[\"var\" + str(i)])\n",
    "        grads[\"dW\" + str(i)] = (1/m) * np.dot(dZ, cache[\"A\" + str(i-1)])\n",
    "        grads[\"db\" + str(i)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dZ_prev = dZ\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def model_mb_bnz(X, Y, layers, bnz=0, num_iterations=100, learning_rate=0.001, mini_batch_size=100):\n",
    "    parameters,bn_parameters = initialize_parameters_bnz(layers)\n",
    "    num_layers = np.size(layers)-1\n",
    "    num_mini_batches = int(X.shape[0]/mini_batch_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        shfl = np.arange(num_mini_batches).astype(int)\n",
    "        np.random.shuffle(shfl)\n",
    "        cost = 0\n",
    "        for j in range(num_mini_batches):\n",
    "            batch_idx = shfl[j]\n",
    "            X_batch = X[batch_idx*mini_batch_size:(batch_idx+1)*mini_batch_size,:]\n",
    "            Y_batch = Y[batch_idx*mini_batch_size:(batch_idx+1)*mini_batch_size,:]\n",
    "            \n",
    "            cache, bn_parameters = forward_pass_bnz(X_batch, parameters, num_layers, bn_parameters, bnz)\n",
    "            grads = backward_pass_bnz(Y_batch, parameters, cache, num_layers, bn_parameters)\n",
    "            parameters = update_weights(parameters, grads, learning_rate, num_layers)\n",
    "            \n",
    "            cost += compute_cost(cache[\"A\"+str(num_layers)], Y_batch)\n",
    "        \n",
    "        if(0 == ((i+1)%100)):\n",
    "            print(\"Iteration: %d, Cost: %.3f\", i+1, cost/num_mini_batches)\n",
    "    \n",
    "    return parameters, bn_parameters"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def predictions_bnz(labels, data, params, num_layers, bn_parameters):\n",
    "    cache, _ = forward_pass_bnz(data, params, num_layers, bn_parameters, 0)\n",
    "    preds = cache[\"A\"+str(num_layers)]\n",
    "    #preds = np.random.randn(preds.shape[0], preds.shape[1])\n",
    "\n",
    "    pm = np.amax(preds, axis=1, keepdims=True)\n",
    "    t = (preds == pm).astype(int)\n",
    "        \n",
    "    a = np.sum((t != labels).astype(int))\n",
    "    accuracy = 1 - (a/(2*labels.shape[0]))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_w(shape):\n",
    "    return (np.random.randn(shape[0],shape[1])*0.1)\n",
    "\n",
    "def initialize_b(shape):\n",
    "    return np.zeros(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layers):\n",
    "    parameters = {}\n",
    "    np.random.seed(10)\n",
    "    num_layers = np.size(layers) - 1\n",
    "    for i in range(num_layers):\n",
    "        parameters[\"W\" + str(i+1)] = initialize_w([layers[i+1], layers[i]])\n",
    "        parameters[\"b\" + str(i+1)] = initialize_b([layers[i+1], 1])\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_z(X, W, b):\n",
    "    Z = np.dot(W, X.T) + b\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_activation_fn(Z, type, dropout=0):\n",
    "    if(type == 'relu'):\n",
    "        A = np.maximum(0,Z)\n",
    "        if(dropout):\n",
    "            drop_rows = np.random.randn(A.shape[0],1)\n",
    "            mid_point = np.mean(drop_rows)\n",
    "            drop_rows = np.where(drop_rows>mid_point, 1, 0)\n",
    "            A = np.multiply(A, drop_rows)\n",
    "                \n",
    "    else: #softmax\n",
    "        denom = np.sum(np.exp(Z), axis=0, keepdims=True)\n",
    "        A = np.exp(Z) / (denom)\n",
    "        \n",
    "    return A.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_relu_grad(Z):\n",
    "    grad = np.where(Z>0, 1, 0)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, parameters, num_layers, dropout=0):\n",
    "    cache = {}\n",
    "    cache[\"A0\"] = X\n",
    "    A_prev = X\n",
    "    \n",
    "    for i in range(num_layers-1):        \n",
    "        cache[\"Z\" + str(i+1)] = calc_z(A_prev, parameters[\"W\" + str(i+1)], parameters[\"b\" + str(i+1)])\n",
    "        cache[\"A\" + str(i+1)] = calc_activation_fn(cache[\"Z\" + str(i+1)], \"relu\", dropout)\n",
    "        A_prev = cache[\"A\" + str(i+1)]\n",
    "    \n",
    "    cache[\"Z\" + str(num_layers)] = calc_z(A_prev, parameters[\"W\" + str(num_layers)], parameters[\"b\" + str(num_layers)])    \n",
    "    cache[\"A\" + str(num_layers)] = calc_activation_fn(cache[\"Z\" + str(num_layers)], \"softmax\")\n",
    "    \n",
    "    return cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(Y, parameters, cache, num_layers):\n",
    "    \n",
    "    m = Y.shape[0]\n",
    "    grads = {}\n",
    "    \n",
    "    dZ_prev = np.transpose(cache[\"A\" + str(num_layers)] - Y)\n",
    "    grads[\"dW\" + str(num_layers)] = (1/m) * np.dot(dZ_prev, cache[\"A\" + str(num_layers-1)])\n",
    "    grads[\"db\" + str(num_layers)] = (1/m) * np.sum(dZ_prev, axis=1, keepdims=True)\n",
    "    \n",
    "    for i in range(num_layers-1, 0, -1):\n",
    "        dZ = np.transpose(np.dot(dZ_prev.T, parameters[\"W\" + str(i+1)])) * compute_relu_grad(cache[\"Z\" + str(i)])\n",
    "        grads[\"dW\" + str(i)] = (1/m) * np.dot(dZ, cache[\"A\" + str(i-1)])\n",
    "        grads[\"db\" + str(i)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dZ_prev = dZ\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_weights(parameters, grads, learning_rate, num_layers):\n",
    "    for i in range(num_layers):\n",
    "        parameters[\"W\" + str(i+1)] -= learning_rate*grads[\"dW\" + str(i+1)]\n",
    "        parameters[\"b\" + str(i+1)] -= learning_rate*grads[\"db\" + str(i+1)]\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Ypred, Y):\n",
    "    m = Y.shape[0]\n",
    "    cost = -(1/m) * np.sum(np.multiply(Y, np.log(Ypred+1e-10)) + np.multiply((1-Y), np.log(1-Ypred+1e-10)))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def model(X, Y, layers, num_iterations=100, learning_rate=0.001):\n",
    "    parameters = initialize_parameters(layers)\n",
    "    num_layers = np.size(layers) - 1\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        cache = forward_pass(X, parameters, num_layers)\n",
    "        grads = backward_pass(Y, parameters, cache, num_layers)\n",
    "        parameters = update_weights(parameters, grads, learning_rate, num_layers)\n",
    "        if(0 == (i%100)):\n",
    "            cost = compute_cost(cache[\"A\"+str(num_layers)], Y)\n",
    "            print(\"Iteration: %d, Cost: %.3f\", i, cost)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_mb(X, Y, layers, num_iterations=100, learning_rate=0.001, mini_batch_size=100, dropout=0):\n",
    "    parameters = initialize_parameters(layers)\n",
    "    num_layers = np.size(layers)-1\n",
    "    num_mini_batches = int(X.shape[0]/mini_batch_size)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        shfl = np.arange(num_mini_batches).astype(int)\n",
    "        np.random.shuffle(shfl)\n",
    "        cost = 0\n",
    "        for j in range(num_mini_batches):\n",
    "            batch_idx = shfl[j]\n",
    "            X_batch = X[batch_idx*mini_batch_size:(batch_idx+1)*mini_batch_size,:]\n",
    "            Y_batch = Y[batch_idx*mini_batch_size:(batch_idx+1)*mini_batch_size,:]\n",
    "            \n",
    "            cache = forward_pass(X_batch, parameters, num_layers, dropout)\n",
    "            grads = backward_pass(Y_batch, parameters, cache, num_layers)\n",
    "            parameters = update_weights(parameters, grads, learning_rate, num_layers)\n",
    "            \n",
    "            cost += compute_cost(cache[\"A\"+str(num_layers)], Y_batch)\n",
    "        \n",
    "        if(0 == ((i+1)%100)):\n",
    "            print(\"Iteration: %d, Cost: %.3f\", i+1, cost/num_mini_batches)\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(labels, data, params, num_layers):\n",
    "    cache = forward_pass(data, params, num_layers)\n",
    "    preds = cache[\"A\"+str(num_layers)]\n",
    "    #preds = np.random.randn(preds.shape[0], preds.shape[1])\n",
    "\n",
    "    pm = np.amax(preds, axis=1, keepdims=True)\n",
    "    t = (preds == pm).astype(int)\n",
    "        \n",
    "    a = np.sum((t != labels).astype(int))\n",
    "    accuracy = 1 - (a/(2*labels.shape[0]))\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 11: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 100 0.643667760508\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 32\n",
    "nl2 = 16\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model_mb(train_data_shfl3, train_target_shfl, layers, num_iterations=100, \\\n",
    "                  learning_rate=0.001, mini_batch_size=250, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88852\n",
      "0.8958\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl3, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data3, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 100 0.341124978776\n",
      "Iteration: %d, Cost: %.3f 200 0.231072942059\n",
      "Iteration: %d, Cost: %.3f 300 0.173907728941\n",
      "Iteration: %d, Cost: %.3f 400 0.135792527549\n",
      "Iteration: %d, Cost: %.3f 500 0.107177961263\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 128\n",
    "nl2 = 64\n",
    "nl3 = 32\n",
    "nl4 = 10\n",
    "layers = [nl0, nl1, nl2, nl3, nl4]\n",
    "params = model_mb(train_data_shfl3, train_target_shfl, layers, num_iterations=500, learning_rate=0.001, mini_batch_size=250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9831\n",
      "0.9619\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl3, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data3, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = pd.read_csv('../Kaggle/MNIST/test.csv').values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images2 = (test_images - np.mean(test_images))/(np.sqrt(np.var(test_images)) + 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = forward_pass(test_images2, params, num_layers)\n",
    "preds = cache[\"A\"+str(num_layers)]\n",
    "kaggle_output = np.argmax(preds, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "np.savetxt('../Kaggle/MNIST/submission.csv', \n",
    "           np.c_[range(1,len(test_images)+1),kaggle_output], \n",
    "           delimiter=',', \n",
    "           header = 'ImageId,Label', \n",
    "           comments = '', \n",
    "           fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 10: Batch normalization (incomplete)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 32\n",
    "nl2 = 16\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params, bn_parameters = model_mb_bnz(train_data_shfl3, train_target_shfl, layers, \\\n",
    "                                 bnz=0, num_iterations=100, learning_rate=0.001, mini_batch_size=250)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions_bnz(train_target_shfl, train_data_shfl3, params, num_layers, bn_parameters)\n",
    "print(accuracy)\n",
    "accuracy = predictions_bnz(test_target, test_data3, params, num_layers, bn_parameters)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 9: Changed hyper parameters to save time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 100 0.643667760508\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 32\n",
    "nl2 = 16\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model_mb(train_data_shfl3, train_target_shfl, layers, 100, 0.001, 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88852\n",
      "0.8958\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl3, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data3, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 8: Mini-batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 0 1.22120072629\n",
      "Iteration: %d, Cost: %.3f 100 0.00330191896417\n",
      "Iteration: %d, Cost: %.3f 200 0.000677789697588\n",
      "Iteration: %d, Cost: %.3f 300 0.000338467524802\n",
      "Iteration: %d, Cost: %.3f 400 0.000217997485401\n",
      "Iteration: %d, Cost: %.3f 500 0.000157799447052\n",
      "Iteration: %d, Cost: %.3f 600 0.000122214119799\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 32\n",
    "nl2 = 16\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model_mb(train_data_shfl3, train_target_shfl, layers, 700, 0.05, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9636\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl3, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data3, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 7: smaller network, less iterations (to reduce computation time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 0 3.28322205529\n",
      "Iteration: %d, Cost: %.3f 100 1.52498070491\n",
      "Iteration: %d, Cost: %.3f 200 0.925394513908\n",
      "Iteration: %d, Cost: %.3f 300 0.735371843318\n",
      "Iteration: %d, Cost: %.3f 400 0.642561162086\n",
      "Iteration: %d, Cost: %.3f 500 0.58322147419\n",
      "Iteration: %d, Cost: %.3f 600 0.540691518373\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 32\n",
    "nl2 = 16\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model(train_data_shfl3, train_target_shfl, layers, 700, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91178\n",
      "0.9162\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl3, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data3, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 6: Unit std dev (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 0 3.32719518049\n",
      "Iteration: %d, Cost: %.3f 100 0.920747641883\n",
      "Iteration: %d, Cost: %.3f 200 0.667183892741\n",
      "Iteration: %d, Cost: %.3f 300 0.569935887113\n",
      "Iteration: %d, Cost: %.3f 400 0.51119767978\n",
      "Iteration: %d, Cost: %.3f 500 0.468540201849\n",
      "Iteration: %d, Cost: %.3f 600 0.434777414317\n",
      "Iteration: %d, Cost: %.3f 700 0.406875158972\n",
      "Iteration: %d, Cost: %.3f 800 0.383157315886\n",
      "Iteration: %d, Cost: %.3f 900 0.362787210359\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 64\n",
    "nl2 = 32\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model(train_data_shfl3, train_target_shfl, layers, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9401\n",
      "0.9397\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl3, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data3, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 5: Unit variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 0 4.04739617434\n",
      "Iteration: %d, Cost: %.3f 100 0.702949176404\n",
      "Iteration: %d, Cost: %.3f 200 0.515706832665\n",
      "Iteration: %d, Cost: %.3f 300 0.436034542052\n",
      "Iteration: %d, Cost: %.3f 400 0.385267458556\n",
      "Iteration: %d, Cost: %.3f 500 0.348151210512\n",
      "Iteration: %d, Cost: %.3f 600 0.319520331222\n",
      "Iteration: %d, Cost: %.3f 700 0.296176147099\n",
      "Iteration: %d, Cost: %.3f 800 0.276604907786\n",
      "Iteration: %d, Cost: %.3f 900 0.259939585609\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 64\n",
    "nl2 = 32\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model(train_data_shfl3, train_target_shfl, layers, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95818\n",
      "0.9508\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl3, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data3, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 4: Add another layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 0 3.25586302648\n",
      "Iteration: %d, Cost: %.3f 100 2.72440423418\n",
      "Iteration: %d, Cost: %.3f 200 1.18542395667\n",
      "Iteration: %d, Cost: %.3f 300 0.799798939977\n",
      "Iteration: %d, Cost: %.3f 400 0.670901248039\n",
      "Iteration: %d, Cost: %.3f 500 0.600259972392\n",
      "Iteration: %d, Cost: %.3f 600 0.551252278373\n",
      "Iteration: %d, Cost: %.3f 700 0.512885402917\n",
      "Iteration: %d, Cost: %.3f 800 0.48096523522\n",
      "Iteration: %d, Cost: %.3f 900 0.453369933814\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 128\n",
    "nl2 = 64\n",
    "nl3 = 32\n",
    "nl4 = 10\n",
    "layers = [nl0, nl1, nl2, nl3, nl4]\n",
    "params = model(train_data_shfl2, train_target_shfl, layers, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9264\n",
      "0.9262\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl2, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data2, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 3: Zero mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 0 3.25593657005\n",
      "Iteration: %d, Cost: %.3f 100 2.30840618614\n",
      "Iteration: %d, Cost: %.3f 200 1.1881943244\n",
      "Iteration: %d, Cost: %.3f 300 0.864869208823\n",
      "Iteration: %d, Cost: %.3f 400 0.730145121645\n",
      "Iteration: %d, Cost: %.3f 500 0.656298411581\n",
      "Iteration: %d, Cost: %.3f 600 0.607328476808\n",
      "Iteration: %d, Cost: %.3f 700 0.571203471051\n",
      "Iteration: %d, Cost: %.3f 800 0.542487802948\n",
      "Iteration: %d, Cost: %.3f 900 0.518415656222\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 64\n",
    "nl2 = 32\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model(train_data_shfl2, train_target_shfl, layers, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91354\n",
      "0.9172\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl2, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data2, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 2: Shuffled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 0 3.26419535528\n",
      "Iteration: %d, Cost: %.3f 100 2.33337878448\n",
      "Iteration: %d, Cost: %.3f 200 1.16612822578\n",
      "Iteration: %d, Cost: %.3f 300 0.855196248566\n",
      "Iteration: %d, Cost: %.3f 400 0.729524977565\n",
      "Iteration: %d, Cost: %.3f 500 0.660332431973\n",
      "Iteration: %d, Cost: %.3f 600 0.613826190547\n",
      "Iteration: %d, Cost: %.3f 700 0.578924857411\n",
      "Iteration: %d, Cost: %.3f 800 0.550779046348\n",
      "Iteration: %d, Cost: %.3f 900 0.52690506348\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 64\n",
    "nl2 = 32\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model(train_data_shfl, train_target_shfl, layers, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91328\n",
      "0.9184\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target_shfl, train_data_shfl, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case 1: Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: %d, Cost: %.3f 0 3.26419535528\n",
      "Iteration: %d, Cost: %.3f 100 2.33337878448\n",
      "Iteration: %d, Cost: %.3f 200 1.16612822578\n",
      "Iteration: %d, Cost: %.3f 300 0.855196248566\n",
      "Iteration: %d, Cost: %.3f 400 0.729524977565\n",
      "Iteration: %d, Cost: %.3f 500 0.660332431973\n",
      "Iteration: %d, Cost: %.3f 600 0.613826190547\n",
      "Iteration: %d, Cost: %.3f 700 0.578924857411\n",
      "Iteration: %d, Cost: %.3f 800 0.550779046348\n",
      "Iteration: %d, Cost: %.3f 900 0.52690506348\n"
     ]
    }
   ],
   "source": [
    "nl0 = 28*28\n",
    "nl1 = 64\n",
    "nl2 = 32\n",
    "nl3 = 10\n",
    "layers = [nl0, nl1, nl2, nl3]\n",
    "params = model(train_data, train_target, layers, 1000, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.91328\n",
      "0.9184\n"
     ]
    }
   ],
   "source": [
    "num_layers = np.size(layers) - 1\n",
    "accuracy = predictions(train_target, train_data, params, num_layers)\n",
    "print(accuracy)\n",
    "accuracy = predictions(test_target, test_data, params, num_layers)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
