
Overfit - function of number of observtions and features

Ridge objective:
  Modified cost = measure of fit (RSS) + measure of magnitude of coefficients

Measure of magnitude of coefficients:
  L1 norm: sum(abs weights)
  L2 norm: sum(weight squares)

cost = RSS(W) + lambda*L2
  lambda controls bias-variance tradeoff
  Large lambda: High bias, low variance
  Small lambda: Low bias, high variance

cost = (y - HW)'(y - HW) + lambda*w'w

Method 1: closed form solution (set gradient to 0)
W = (H'H + lambda*I)^-1 * H'y
(H'H + lambda*I) is always invertible => advantage of ridge (lambda*I is making H'H more "regular" -> regularization)
Do not penalize the intercept term

Method 2: gradient descent
Wj_t+1 <- (1 - 2*step_size*lambda)*Wj_t + 2*step_size*sum(hj(xi)(yi - pred))

Leave one out cross validation

k-fold cross validation (when there is not sufficient data):
  Estimate W on training blocks
  Compute error on validation block
    Change vaidation block, repeat above
    Calculate average error
  Repeat procedure for each lambda
  Choose lambda with lowest average erro
