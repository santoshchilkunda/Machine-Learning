
1) How to represent a document?
  * Bag of words model
  * Term Frequency, Inverse Document Frequency (TF-IDF)
    TF = word count
    IDF = log (#docs / (1 + #doc_using_the_word))

2) How to measure similarity?
  * Euclidean distance
    sqrt((Xi - Xq)'(Xi-Xq))
  * Scaled euclidean distance: Higher weight for important words
    sqrt((Xi - Xq)'A(Xi-Xq))
    A - diagonal matrix of weights
  * Cosine similarity (inner product)
    Xi'Xq (= cos theta)
    Normalization: If unnormalized, doubling the vectors will increase the distance 4X
    Cosine distance = (1 - normalized cosine similarity) = (1 - (Xi'Xq)/(||Xi||*||Xq||))
    Might return the same distance for short tweet and a long article

3) How to search over all documents?
  * 1 - NN
  * k - NN

Complexity of brute force k-NN:
1-NN: O(N) distance computations per 1-NN query
k-NN: O(N*log k)

KD trees
 data structure to represent data
 recursively partition data into axis aligned boxes

KD tree construction
 Split into two groups based on xi
 Split each group based on xj next

Which dimension to split on? widest, alternating
Value to split at? average, median
When to stop? fewer than m points, reaches minimum width

Pruning: calculate the distance to the tight boundary

Complexity of KD tree
 Construction: O(N*log N)
 1-NN query: O(log N) [max pruning] to O(N) [no pruning]
 k-NN query:O(N*log N) to O(N^2)

Locality Sensitive Hashing (LSH) for approximate NN search
 Provides only approx NN
 There might still be too many points in the bin to search over
 Using a random line might work
 Reduce search cost through more bins
  Sacrificing accuracy for speed - greater chance that searching in a single bin is not going to give the best result
  Improve search result by searching adjacent bins

LSH construction
 Draw h random lines
 Compute score (-1 -> 0, +1 -> 1) for each point under each line and translate to binary index
 Use the h-bit bitvector per datapoint as bin index
 Create hash table (bin indices -> keys)
 For each query point x, search bin(x) and neighboring bins until time limit

Cost: d (sparse) multiplications per datapoint

Multiple hash tables better than searching more bins in a single hash table
Multiple hash tables approach - cost of creating tables in higher but offers more accuracy

PROGRAMMING NOTES

# csr (compressed sparse row) matrix
from scipy.sparse import csr_matrix

# count vectorizer, tfidf vectorizer
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

# add row number
wiki['row_number'] = wiki.index # pandas
wiki = wiki.add_row_number() # graphlab

# nearest neightbors
from sklearn.neighbors import NearestNeighbors
fit, kneighbors

# flatten, join
neighbors = graphlab.SFrame({'distance':bo_knn_dist.flatten(), 'id':bo_knn_idx.flatten()})
print wiki.join(neighbors, on='id').sort('distance')[['id','name','distance']]

# split 'word_count' into 'word' and 'count'
word_count_table = row[['word_count']].stack('word_count', new_column_name=['word','count'])

# combine two arrays on 'word'
combined_words = obama_words.join(barrio_words, on='word')

# rename columns
combined_words = combined_words.rename({'count':'Obama', 'count.1':'Barrio'})

# subset check
common_words = set(combined_words['word'][0:5])
unique_words = (word_count_vector.keys())
common_words.issubset(unique_words)

# euclidean and cosine distance metric
from sklearn.metrics.pairwise import euclidean_distances
from sklearn.metrics.pairwise import cosine_distances

# to return a bitvector
np.array(doc.dot(random_vectors) >= 0, dtype=int)

# convert bitvector to corresponding integer
powers_of_two = (1 << np.arange(15, -1, -1))
index_bits.dot(powers_of_two)

# hash table
table = {}
table[bin_index] = [data_index]
values = table[bin_index]
values.append(data_index)
table[bin_index] = values

# get all combinations
from itertools import combinations
num_vector=16
search_radius=3
for diff in combinations(range(num_vector), search_radius):
 print diff
